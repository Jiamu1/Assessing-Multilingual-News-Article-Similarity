{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qnz6KaNPZwB"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MooPVMEe-6GM",
        "outputId": "ee42e75e-e9eb-42cc-aeb0-2bd7b820636a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l1kmuYY_I96",
        "outputId": "50f74c09-8b05-4be1-91cd-f273047bb746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: googletrans==4.0.0rc1 in /usr/local/lib/python3.10/dist-packages (4.0.0rc1)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==4.0.0rc1) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (2022.12.7)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (2023.1.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (1.3.0)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1) (3.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install googletrans==4.0.0rc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yycOJ9QQ-wEL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import json\n",
        "import time\n",
        "from google.colab import drive\n",
        "import random\n",
        "import regex as re\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sentencepiece\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AdamW\n",
        "from transformers import XLMRobertaConfig, XLMRobertaModel, XLMRobertaTokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import googletrans\n",
        "from googletrans import Translator\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY6di9hn-0z6",
        "outputId": "e6b5606b-7802-49d9-c0e6-a7e0b1e2b170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive\n",
            "/content/drive/MyDrive/CS263/Final Project/Extracted_data\n"
          ]
        }
      ],
      "source": [
        "FOLDERNAME = '/content/drive/MyDrive/CS263/Final Project/Extracted_data'\n",
        "%cd drive/My\\ Drive\n",
        "%cd $FOLDERNAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FDs6YK8QBEnp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Zp1PbVYsMvyO"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"train_df.csv\")\n",
        "dev_data = pd.read_csv(\"evaluation_df.csv\")\n",
        "\n",
        "# Drop rows with NaN values in 'text1' or 'text2'\n",
        "# train = train_data.dropna()\n",
        "# dev = dev_data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "s6vWS4x6E63o"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWF4HhpGSBke",
        "outputId": "27780a8d-d699-4b43-ecc7-effef148791d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After removing NA text columns, we lose 113 rows.\n"
          ]
        }
      ],
      "source": [
        "# Delete columns that contain NaN sentences.\n",
        "processed_data = train_data[train_data['text1'].notna()]\n",
        "processed_data = processed_data[processed_data['text2'].notna()]\n",
        "\n",
        "print(\"After removing NA text columns, we lose {0} rows.\".format(train_data.shape[0] - processed_data.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zbrjlvK0D9H7"
      },
      "outputs": [],
      "source": [
        "processed_data = processed_data.head(500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-F56wV3MSbfV"
      },
      "outputs": [],
      "source": [
        "# split into train and development.\n",
        "train, dev = train_test_split(processed_data, test_size=0.1, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "0Tmz5XDvSiID",
        "outputId": "0f2f0c51-6914-4f20-c62a-8dda13b6e83e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    url1_lang url2_lang                pair_id  \\\n",
              "72         en        en  1498601317_1483894995   \n",
              "182        en        en  1484452449_1483829079   \n",
              "\n",
              "                                                 link1  \\\n",
              "72   https://plainsman.com/article/2019-big-bo-classic   \n",
              "182  https://timesofindia.indiatimes.com/city/luckn...   \n",
              "\n",
              "                                                 link2  \\\n",
              "72   http://www.nigerianeye.com/2020/01/2020mfm-go-...   \n",
              "182  https://in-cyprus.com/weather-forecast-rain-an...   \n",
              "\n",
              "                                              ia_link1  \\\n",
              "72   https://web.archive.org/web/plainsman.com/arti...   \n",
              "182  https://web.archive.org/web/timesofindia.india...   \n",
              "\n",
              "                                              ia_link2  Geography  Entities  \\\n",
              "72   https://web.archive.org/web/www.nigerianeye.co...        4.0  4.000000   \n",
              "182  https://web.archive.org/web/in-cyprus.com/weat...        4.0  2.333333   \n",
              "\n",
              "         Time  Narrative  Overall     Style      Tone  \\\n",
              "72   2.333333   4.000000      4.0  3.666667  2.666667   \n",
              "182  1.000000   1.666667      2.0  1.000000  1.333333   \n",
              "\n",
              "                                                 text1  \\\n",
              "72   Thursday, June 2, 2022 \\nAlready a subscriber?...   \n",
              "182                                       Unnamed: 182   \n",
              "\n",
              "                                                 text2  \n",
              "72   Signup for FREE news updates, latest informati...  \n",
              "182  Today there will be isolated showers island-wi...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8fc02b6c-73ec-4a18-b9e9-a73a48d4ae59\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url1_lang</th>\n",
              "      <th>url2_lang</th>\n",
              "      <th>pair_id</th>\n",
              "      <th>link1</th>\n",
              "      <th>link2</th>\n",
              "      <th>ia_link1</th>\n",
              "      <th>ia_link2</th>\n",
              "      <th>Geography</th>\n",
              "      <th>Entities</th>\n",
              "      <th>Time</th>\n",
              "      <th>Narrative</th>\n",
              "      <th>Overall</th>\n",
              "      <th>Style</th>\n",
              "      <th>Tone</th>\n",
              "      <th>text1</th>\n",
              "      <th>text2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>1498601317_1483894995</td>\n",
              "      <td>https://plainsman.com/article/2019-big-bo-classic</td>\n",
              "      <td>http://www.nigerianeye.com/2020/01/2020mfm-go-...</td>\n",
              "      <td>https://web.archive.org/web/plainsman.com/arti...</td>\n",
              "      <td>https://web.archive.org/web/www.nigerianeye.co...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.333333</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.666667</td>\n",
              "      <td>2.666667</td>\n",
              "      <td>Thursday, June 2, 2022 \\nAlready a subscriber?...</td>\n",
              "      <td>Signup for FREE news updates, latest informati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>1484452449_1483829079</td>\n",
              "      <td>https://timesofindia.indiatimes.com/city/luckn...</td>\n",
              "      <td>https://in-cyprus.com/weather-forecast-rain-an...</td>\n",
              "      <td>https://web.archive.org/web/timesofindia.india...</td>\n",
              "      <td>https://web.archive.org/web/in-cyprus.com/weat...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.333333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.333333</td>\n",
              "      <td>Unnamed: 182</td>\n",
              "      <td>Today there will be isolated showers island-wi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8fc02b6c-73ec-4a18-b9e9-a73a48d4ae59')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8fc02b6c-73ec-4a18-b9e9-a73a48d4ae59 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8fc02b6c-73ec-4a18-b9e9-a73a48d4ae59');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "c8CCVtsoB3RN"
      },
      "outputs": [],
      "source": [
        "# class SiameseNetwork(nn.Module):\n",
        "#     def __init__(self, bert_model):\n",
        "#         super(SiameseNetwork, self).__init__()\n",
        "#         self.bert_model = bert_model\n",
        "#         # self.fc = nn.Linear(bert_model.config.hidden_size, 1)\n",
        "\n",
        "#     def forward(self, text_inputs, attention_mask):\n",
        "#         output = self.bert_model(text_inputs, attention_mask=attention_mask)\n",
        "#         # pooled_output = outputs.pooler_output\n",
        "#         # output = self.fc(pooled_output)\n",
        "#         return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sonzf2vC_Qo"
      },
      "source": [
        "#XLM-BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eyqz1SN8J89j"
      },
      "outputs": [],
      "source": [
        "max_len = 512\n",
        "batch_size = 5\n",
        "lr = 5e-6\n",
        "weight_decay = 1e-4\n",
        "num_epochs = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BkyWa_UOJKuP"
      },
      "outputs": [],
      "source": [
        "\n",
        "device = torch.device(\"cuda\")\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "#set_seed(self.config.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JTzOjywWLjHf"
      },
      "outputs": [],
      "source": [
        "def get_data_loader(data, batch_size_flg = True):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "  input_ids, attention_masks, labels = [], [], []\n",
        "  for idx, row in data.iterrows():\n",
        "      text1, text2 = row['text1'], row['text2']\n",
        "      encode_dict = tokenizer(text1,text2,\n",
        "                                  max_length=max_len,\n",
        "                                  padding='max_length',\n",
        "                                  truncation=True,\n",
        "                                  add_special_tokens=True\n",
        "                                  )\n",
        "      \n",
        "      input_ids.append(encode_dict['input_ids'])\n",
        "      attention_masks.append(encode_dict['attention_mask'])\n",
        "      # Convert to only 1 label.\n",
        "      labels.append([float(x) for x in [row['Geography'],row['Entities'],row['Time'],row['Narrative'],row['Overall'],row['Style'],row['Tone']]])\n",
        "\n",
        "  input_ids = torch.tensor(input_ids)\n",
        "  attention_masks = torch.tensor(attention_masks)\n",
        "  labels = torch.tensor(labels)\n",
        "\n",
        "  data = TensorDataset(input_ids, attention_masks, labels)\n",
        "  if(batch_size_flg):\n",
        "      data_loader = DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "  else:\n",
        "      data_loader = DataLoader(data)\n",
        "  return data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LHXXAcnbNEBP"
      },
      "outputs": [],
      "source": [
        "train_data_loader = get_data_loader(train)\n",
        "eval_data_loader = get_data_loader(dev, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-7G7LuxOLKGE"
      },
      "outputs": [],
      "source": [
        "class MMRegressor(nn.Module):\n",
        "\n",
        "    def __init__(self, model, hidden_size):\n",
        "        \n",
        "        super(MMRegressor, self).__init__()\n",
        "        #self.config = XLMRobertaConfig.from_pretrained()\n",
        "        self.reg_model = model\n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_size, 512)\n",
        "        self.fc2 = nn.Linear(512, 7)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "\n",
        "        output1 = self.reg_model(input_ids, attention_mask)[1]\n",
        "        logits1 = self.fc2(self.activation(self.fc1(output1)))\n",
        "\n",
        "        output2 = self.reg_model(input_ids, attention_mask)[1]\n",
        "        logits2 = self.fc2(self.activation(self.fc1(output2)))\n",
        "        \n",
        "        return logits1, logits2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "I_MArhgVPFWs"
      },
      "outputs": [],
      "source": [
        "def predict(model, data_loader):\n",
        "\n",
        "    model.eval()\n",
        "    test_pred, test_true = [], []\n",
        "    with torch.no_grad():\n",
        "        for idx, (ids, att, y) in enumerate(data_loader):\n",
        "            y_pred = model(ids.to(device), att.to(device))\n",
        "            y_pred = torch.squeeze(torch.add(torch.mul(y_pred[0], 0.5), torch.mul(y_pred[1], 0.5))).detach().cpu().numpy().tolist()\n",
        "            y = y.squeeze().cpu().numpy().tolist()\n",
        "\n",
        "            test_true.append(y[4])\n",
        "            test_pred.append(y_pred[4])\n",
        "\n",
        "        return test_true, test_pred\n",
        "\n",
        "\n",
        "def calculate_weighted_loss( y_pred, y, criterion, loss_weights):\n",
        "  loss = 0.0\n",
        "  for i in range(7):\n",
        "    y_pred_i, y_i = y_pred[:, i], y[:, i]\n",
        "    loss += criterion(y_pred_i, y_i) * loss_weights[i]\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5K_xyU5kP9v_"
      },
      "outputs": [],
      "source": [
        "def train(model, model_path, train_loader, valid_loader, optimizer, epoches, loss_weights):\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    best_pearson = 0.0\n",
        "    criterion = nn.MSELoss()\n",
        "    model.train()\n",
        "\n",
        "    for i in range(epoches):\n",
        "        start_time = time.time()\n",
        "        train_loss_sum = 0.0\n",
        "        \n",
        "        print(f\"—————————————————————— Epoch {i+1} ——————————————————————\")\n",
        "        #print(logging.info)\n",
        "        \n",
        "        for idx, (ids, att, y) in enumerate(train_loader):\n",
        "\n",
        "            ids, att, y = ids.to(device), att.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred1, y_pred2 = model(ids, att)\n",
        "            y_pred1, y_pred2, y = torch.squeeze(y_pred1), torch.squeeze(y_pred2), torch.squeeze(y)\n",
        "\n",
        "            loss1 = calculate_weighted_loss(y_pred1, y, criterion, loss_weights) #* self.config.losses_weights['forward_weight']\n",
        "            loss2 = calculate_weighted_loss(y_pred2, y, criterion, loss_weights) #* self.config.losses_weights['forward_weight']\n",
        "            loss_r = calculate_weighted_loss(y_pred1, y_pred2, criterion, loss_weights) #* self.config.losses_weights['rdrop_weight']     \n",
        "            loss = (loss1 + loss2 + loss_r) /3\n",
        "\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            #schedule.step()\n",
        "            train_loss_sum += loss.item()\n",
        "\n",
        "            if (idx+1) % (len(train_loader) // 10) == 0:\n",
        "                print(\"Epoch {:02d} | Step {:03d}/{:03d} | Loss {:.4f} | Time {:.2f}\".format(i+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time()-start_time))\n",
        "        \n",
        "\n",
        "        print(\"Start evaluating!\")\n",
        "        dev_true, dev_pred = predict(model, valid_loader)\n",
        "        cur_pearson = np.corrcoef(dev_true, dev_pred)[0][1]\n",
        "        \n",
        "        if cur_pearson > best_pearson:\n",
        "            best_pearson = cur_pearson\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "        \n",
        "        print(\"Current dev pearson is {:.4f}, best pearson is {:.4f}\".format(cur_pearson, best_pearson))\n",
        "        print(\"Time costed : {}s \\n\".format(round(time.time() - start_time, 3)))\n",
        "        #print('logging',logging.info)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainning"
      ],
      "metadata": {
        "id": "TPL190T3v_CN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU-yXyyqQh_j",
        "outputId": "5b704ab2-6742-4d70-a746-538c7632c8fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model name for this run: XLM_Roberta_base.pth\n",
            "—————————————————————— Epoch 1 ——————————————————————\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Step 009/090 | Loss 5.1406 | Time 3.23\n",
            "Epoch 01 | Step 018/090 | Loss 4.9923 | Time 5.49\n",
            "Epoch 01 | Step 027/090 | Loss 4.9280 | Time 7.75\n",
            "Epoch 01 | Step 036/090 | Loss 4.8962 | Time 10.01\n",
            "Epoch 01 | Step 045/090 | Loss 4.8296 | Time 12.28\n",
            "Epoch 01 | Step 054/090 | Loss 4.6085 | Time 14.54\n",
            "Epoch 01 | Step 063/090 | Loss 4.4226 | Time 16.80\n",
            "Epoch 01 | Step 072/090 | Loss 4.2765 | Time 19.06\n",
            "Epoch 01 | Step 081/090 | Loss 4.1730 | Time 21.32\n",
            "Epoch 01 | Step 090/090 | Loss 4.0119 | Time 23.59\n",
            "Start evaluating!\n",
            "Current dev pearson is -0.1619, best pearson is 0.0000\n",
            "Time costed : 24.774s \n",
            "\n",
            "—————————————————————— Epoch 2 ——————————————————————\n",
            "Epoch 02 | Step 009/090 | Loss 2.1715 | Time 2.21\n",
            "Epoch 02 | Step 018/090 | Loss 2.1066 | Time 4.42\n",
            "Epoch 02 | Step 027/090 | Loss 2.1255 | Time 6.62\n",
            "Epoch 02 | Step 036/090 | Loss 2.0307 | Time 8.83\n",
            "Epoch 02 | Step 045/090 | Loss 1.9825 | Time 11.04\n",
            "Epoch 02 | Step 054/090 | Loss 1.9421 | Time 13.25\n",
            "Epoch 02 | Step 063/090 | Loss 1.8806 | Time 15.46\n",
            "Epoch 02 | Step 072/090 | Loss 1.8357 | Time 17.66\n",
            "Epoch 02 | Step 081/090 | Loss 1.7727 | Time 19.87\n",
            "Epoch 02 | Step 090/090 | Loss 1.7010 | Time 22.08\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.4074, best pearson is 0.4074\n",
            "Time costed : 26.18s \n",
            "\n",
            "—————————————————————— Epoch 3 ——————————————————————\n",
            "Epoch 03 | Step 009/090 | Loss 1.0706 | Time 2.23\n",
            "Epoch 03 | Step 018/090 | Loss 1.0059 | Time 4.45\n",
            "Epoch 03 | Step 027/090 | Loss 1.0779 | Time 6.65\n",
            "Epoch 03 | Step 036/090 | Loss 1.0291 | Time 8.86\n",
            "Epoch 03 | Step 045/090 | Loss 0.9954 | Time 11.07\n",
            "Epoch 03 | Step 054/090 | Loss 1.0049 | Time 13.28\n",
            "Epoch 03 | Step 063/090 | Loss 0.9960 | Time 15.49\n",
            "Epoch 03 | Step 072/090 | Loss 0.9870 | Time 17.70\n",
            "Epoch 03 | Step 081/090 | Loss 0.9577 | Time 19.91\n",
            "Epoch 03 | Step 090/090 | Loss 0.9390 | Time 22.12\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.6007, best pearson is 0.6007\n",
            "Time costed : 26.429s \n",
            "\n",
            "—————————————————————— Epoch 4 ——————————————————————\n",
            "Epoch 04 | Step 009/090 | Loss 0.8397 | Time 2.26\n",
            "Epoch 04 | Step 018/090 | Loss 0.8554 | Time 4.47\n",
            "Epoch 04 | Step 027/090 | Loss 0.8268 | Time 6.68\n",
            "Epoch 04 | Step 036/090 | Loss 0.8372 | Time 8.89\n",
            "Epoch 04 | Step 045/090 | Loss 0.8264 | Time 11.09\n",
            "Epoch 04 | Step 054/090 | Loss 0.7941 | Time 13.30\n",
            "Epoch 04 | Step 063/090 | Loss 0.7900 | Time 15.51\n",
            "Epoch 04 | Step 072/090 | Loss 0.7823 | Time 17.72\n",
            "Epoch 04 | Step 081/090 | Loss 0.7747 | Time 19.93\n",
            "Epoch 04 | Step 090/090 | Loss 0.7598 | Time 22.14\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.4990, best pearson is 0.6007\n",
            "Time costed : 23.319s \n",
            "\n",
            "—————————————————————— Epoch 5 ——————————————————————\n",
            "Epoch 05 | Step 009/090 | Loss 0.7101 | Time 2.21\n",
            "Epoch 05 | Step 018/090 | Loss 0.6823 | Time 4.42\n",
            "Epoch 05 | Step 027/090 | Loss 0.7045 | Time 6.63\n",
            "Epoch 05 | Step 036/090 | Loss 0.7059 | Time 8.83\n",
            "Epoch 05 | Step 045/090 | Loss 0.6929 | Time 11.05\n",
            "Epoch 05 | Step 054/090 | Loss 0.6878 | Time 13.25\n",
            "Epoch 05 | Step 063/090 | Loss 0.6860 | Time 15.46\n",
            "Epoch 05 | Step 072/090 | Loss 0.6831 | Time 17.67\n",
            "Epoch 05 | Step 081/090 | Loss 0.6676 | Time 19.88\n",
            "Epoch 05 | Step 090/090 | Loss 0.6715 | Time 22.09\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7040, best pearson is 0.7040\n",
            "Time costed : 26.078s \n",
            "\n",
            "—————————————————————— Epoch 6 ——————————————————————\n",
            "Epoch 06 | Step 009/090 | Loss 0.5221 | Time 2.24\n",
            "Epoch 06 | Step 018/090 | Loss 0.5250 | Time 4.45\n",
            "Epoch 06 | Step 027/090 | Loss 0.5343 | Time 6.66\n",
            "Epoch 06 | Step 036/090 | Loss 0.4994 | Time 8.87\n",
            "Epoch 06 | Step 045/090 | Loss 0.4847 | Time 11.07\n",
            "Epoch 06 | Step 054/090 | Loss 0.4863 | Time 13.28\n",
            "Epoch 06 | Step 063/090 | Loss 0.4750 | Time 15.49\n",
            "Epoch 06 | Step 072/090 | Loss 0.4703 | Time 17.70\n",
            "Epoch 06 | Step 081/090 | Loss 0.4765 | Time 19.91\n",
            "Epoch 06 | Step 090/090 | Loss 0.4695 | Time 22.12\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7660, best pearson is 0.7660\n",
            "Time costed : 26.573s \n",
            "\n",
            "—————————————————————— Epoch 7 ——————————————————————\n",
            "Epoch 07 | Step 009/090 | Loss 0.3862 | Time 2.24\n",
            "Epoch 07 | Step 018/090 | Loss 0.3660 | Time 4.44\n",
            "Epoch 07 | Step 027/090 | Loss 0.3674 | Time 6.65\n",
            "Epoch 07 | Step 036/090 | Loss 0.3533 | Time 8.86\n",
            "Epoch 07 | Step 045/090 | Loss 0.3415 | Time 11.07\n",
            "Epoch 07 | Step 054/090 | Loss 0.3441 | Time 13.28\n",
            "Epoch 07 | Step 063/090 | Loss 0.3341 | Time 15.48\n",
            "Epoch 07 | Step 072/090 | Loss 0.3436 | Time 17.69\n",
            "Epoch 07 | Step 081/090 | Loss 0.3434 | Time 19.90\n",
            "Epoch 07 | Step 090/090 | Loss 0.3490 | Time 22.11\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7646, best pearson is 0.7660\n",
            "Time costed : 23.337s \n",
            "\n",
            "—————————————————————— Epoch 8 ——————————————————————\n",
            "Epoch 08 | Step 009/090 | Loss 0.4116 | Time 2.21\n",
            "Epoch 08 | Step 018/090 | Loss 0.3993 | Time 4.42\n",
            "Epoch 08 | Step 027/090 | Loss 0.3586 | Time 6.63\n",
            "Epoch 08 | Step 036/090 | Loss 0.3521 | Time 8.83\n",
            "Epoch 08 | Step 045/090 | Loss 0.3359 | Time 11.04\n",
            "Epoch 08 | Step 054/090 | Loss 0.3198 | Time 13.25\n",
            "Epoch 08 | Step 063/090 | Loss 0.3087 | Time 15.46\n",
            "Epoch 08 | Step 072/090 | Loss 0.3075 | Time 17.67\n",
            "Epoch 08 | Step 081/090 | Loss 0.2995 | Time 19.88\n",
            "Epoch 08 | Step 090/090 | Loss 0.2997 | Time 22.09\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7684, best pearson is 0.7684\n",
            "Time costed : 26.115s \n",
            "\n",
            "—————————————————————— Epoch 9 ——————————————————————\n",
            "Epoch 09 | Step 009/090 | Loss 0.2327 | Time 2.25\n",
            "Epoch 09 | Step 018/090 | Loss 0.2688 | Time 4.45\n",
            "Epoch 09 | Step 027/090 | Loss 0.2636 | Time 6.66\n",
            "Epoch 09 | Step 036/090 | Loss 0.2572 | Time 8.87\n",
            "Epoch 09 | Step 045/090 | Loss 0.2478 | Time 11.08\n",
            "Epoch 09 | Step 054/090 | Loss 0.2389 | Time 13.28\n",
            "Epoch 09 | Step 063/090 | Loss 0.2451 | Time 15.49\n",
            "Epoch 09 | Step 072/090 | Loss 0.2401 | Time 17.70\n",
            "Epoch 09 | Step 081/090 | Loss 0.2458 | Time 19.91\n",
            "Epoch 09 | Step 090/090 | Loss 0.2471 | Time 22.12\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7393, best pearson is 0.7684\n",
            "Time costed : 23.307s \n",
            "\n",
            "—————————————————————— Epoch 10 ——————————————————————\n",
            "Epoch 10 | Step 009/090 | Loss 0.2912 | Time 2.21\n",
            "Epoch 10 | Step 018/090 | Loss 0.2887 | Time 4.42\n",
            "Epoch 10 | Step 027/090 | Loss 0.2591 | Time 6.63\n",
            "Epoch 10 | Step 036/090 | Loss 0.2410 | Time 8.84\n",
            "Epoch 10 | Step 045/090 | Loss 0.2361 | Time 11.04\n",
            "Epoch 10 | Step 054/090 | Loss 0.2293 | Time 13.25\n",
            "Epoch 10 | Step 063/090 | Loss 0.2234 | Time 15.46\n",
            "Epoch 10 | Step 072/090 | Loss 0.2240 | Time 17.67\n",
            "Epoch 10 | Step 081/090 | Loss 0.2188 | Time 19.88\n",
            "Epoch 10 | Step 090/090 | Loss 0.2164 | Time 22.08\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7717, best pearson is 0.7717\n",
            "Time costed : 26.078s \n",
            "\n",
            "—————————————————————— Epoch 11 ——————————————————————\n",
            "Epoch 11 | Step 009/090 | Loss 0.2000 | Time 2.26\n",
            "Epoch 11 | Step 018/090 | Loss 0.2241 | Time 4.47\n",
            "Epoch 11 | Step 027/090 | Loss 0.2117 | Time 6.67\n",
            "Epoch 11 | Step 036/090 | Loss 0.2001 | Time 8.88\n",
            "Epoch 11 | Step 045/090 | Loss 0.1942 | Time 11.09\n",
            "Epoch 11 | Step 054/090 | Loss 0.1890 | Time 13.29\n",
            "Epoch 11 | Step 063/090 | Loss 0.1876 | Time 15.50\n",
            "Epoch 11 | Step 072/090 | Loss 0.1856 | Time 17.71\n",
            "Epoch 11 | Step 081/090 | Loss 0.1820 | Time 19.91\n",
            "Epoch 11 | Step 090/090 | Loss 0.1820 | Time 22.12\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7590, best pearson is 0.7717\n",
            "Time costed : 23.429s \n",
            "\n",
            "—————————————————————— Epoch 12 ——————————————————————\n",
            "Epoch 12 | Step 009/090 | Loss 0.1647 | Time 2.21\n",
            "Epoch 12 | Step 018/090 | Loss 0.1619 | Time 4.42\n",
            "Epoch 12 | Step 027/090 | Loss 0.1696 | Time 6.63\n",
            "Epoch 12 | Step 036/090 | Loss 0.1783 | Time 8.83\n",
            "Epoch 12 | Step 045/090 | Loss 0.1798 | Time 11.04\n",
            "Epoch 12 | Step 054/090 | Loss 0.1783 | Time 13.25\n",
            "Epoch 12 | Step 063/090 | Loss 0.1765 | Time 15.45\n",
            "Epoch 12 | Step 072/090 | Loss 0.1744 | Time 17.66\n",
            "Epoch 12 | Step 081/090 | Loss 0.1727 | Time 19.87\n",
            "Epoch 12 | Step 090/090 | Loss 0.1714 | Time 22.07\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7566, best pearson is 0.7717\n",
            "Time costed : 23.28s \n",
            "\n",
            "—————————————————————— Epoch 13 ——————————————————————\n",
            "Epoch 13 | Step 009/090 | Loss 0.1427 | Time 2.21\n",
            "Epoch 13 | Step 018/090 | Loss 0.1455 | Time 4.41\n",
            "Epoch 13 | Step 027/090 | Loss 0.1465 | Time 6.62\n",
            "Epoch 13 | Step 036/090 | Loss 0.1579 | Time 8.82\n",
            "Epoch 13 | Step 045/090 | Loss 0.1590 | Time 11.03\n",
            "Epoch 13 | Step 054/090 | Loss 0.1590 | Time 13.23\n",
            "Epoch 13 | Step 063/090 | Loss 0.1635 | Time 15.44\n",
            "Epoch 13 | Step 072/090 | Loss 0.1640 | Time 17.65\n",
            "Epoch 13 | Step 081/090 | Loss 0.1667 | Time 19.86\n",
            "Epoch 13 | Step 090/090 | Loss 0.1680 | Time 22.06\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7443, best pearson is 0.7717\n",
            "Time costed : 23.238s \n",
            "\n",
            "—————————————————————— Epoch 14 ——————————————————————\n",
            "Epoch 14 | Step 009/090 | Loss 0.1608 | Time 2.21\n",
            "Epoch 14 | Step 018/090 | Loss 0.1525 | Time 4.41\n",
            "Epoch 14 | Step 027/090 | Loss 0.1602 | Time 6.62\n",
            "Epoch 14 | Step 036/090 | Loss 0.1633 | Time 8.83\n",
            "Epoch 14 | Step 045/090 | Loss 0.1684 | Time 11.04\n",
            "Epoch 14 | Step 054/090 | Loss 0.1687 | Time 13.25\n",
            "Epoch 14 | Step 063/090 | Loss 0.1646 | Time 15.45\n",
            "Epoch 14 | Step 072/090 | Loss 0.1665 | Time 17.66\n",
            "Epoch 14 | Step 081/090 | Loss 0.1652 | Time 19.87\n",
            "Epoch 14 | Step 090/090 | Loss 0.1671 | Time 22.07\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7671, best pearson is 0.7717\n",
            "Time costed : 23.257s \n",
            "\n",
            "—————————————————————— Epoch 15 ——————————————————————\n",
            "Epoch 15 | Step 009/090 | Loss 0.1570 | Time 2.20\n",
            "Epoch 15 | Step 018/090 | Loss 0.1657 | Time 4.41\n",
            "Epoch 15 | Step 027/090 | Loss 0.1558 | Time 6.62\n",
            "Epoch 15 | Step 036/090 | Loss 0.1530 | Time 8.83\n",
            "Epoch 15 | Step 045/090 | Loss 0.1566 | Time 11.03\n",
            "Epoch 15 | Step 054/090 | Loss 0.1530 | Time 13.24\n",
            "Epoch 15 | Step 063/090 | Loss 0.1557 | Time 15.45\n",
            "Epoch 15 | Step 072/090 | Loss 0.1566 | Time 17.65\n",
            "Epoch 15 | Step 081/090 | Loss 0.1581 | Time 19.86\n",
            "Epoch 15 | Step 090/090 | Loss 0.1596 | Time 22.07\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7581, best pearson is 0.7717\n",
            "Time costed : 23.255s \n",
            "\n",
            "—————————————————————— Epoch 16 ——————————————————————\n",
            "Epoch 16 | Step 009/090 | Loss 0.1671 | Time 2.21\n",
            "Epoch 16 | Step 018/090 | Loss 0.1671 | Time 4.41\n",
            "Epoch 16 | Step 027/090 | Loss 0.1637 | Time 6.62\n",
            "Epoch 16 | Step 036/090 | Loss 0.1608 | Time 8.83\n",
            "Epoch 16 | Step 045/090 | Loss 0.1537 | Time 11.03\n",
            "Epoch 16 | Step 054/090 | Loss 0.1523 | Time 13.24\n",
            "Epoch 16 | Step 063/090 | Loss 0.1528 | Time 15.45\n",
            "Epoch 16 | Step 072/090 | Loss 0.1509 | Time 17.66\n",
            "Epoch 16 | Step 081/090 | Loss 0.1509 | Time 19.86\n",
            "Epoch 16 | Step 090/090 | Loss 0.1529 | Time 22.07\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7679, best pearson is 0.7717\n",
            "Time costed : 23.262s \n",
            "\n",
            "—————————————————————— Epoch 17 ——————————————————————\n",
            "Epoch 17 | Step 009/090 | Loss 0.1619 | Time 2.21\n",
            "Epoch 17 | Step 018/090 | Loss 0.1543 | Time 4.41\n",
            "Epoch 17 | Step 027/090 | Loss 0.1590 | Time 6.62\n",
            "Epoch 17 | Step 036/090 | Loss 0.1620 | Time 8.82\n",
            "Epoch 17 | Step 045/090 | Loss 0.1577 | Time 11.03\n",
            "Epoch 17 | Step 054/090 | Loss 0.1499 | Time 13.24\n",
            "Epoch 17 | Step 063/090 | Loss 0.1492 | Time 15.44\n",
            "Epoch 17 | Step 072/090 | Loss 0.1506 | Time 17.65\n",
            "Epoch 17 | Step 081/090 | Loss 0.1521 | Time 19.85\n",
            "Epoch 17 | Step 090/090 | Loss 0.1508 | Time 22.06\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7697, best pearson is 0.7717\n",
            "Time costed : 23.301s \n",
            "\n",
            "—————————————————————— Epoch 18 ——————————————————————\n",
            "Epoch 18 | Step 009/090 | Loss 0.1332 | Time 2.21\n",
            "Epoch 18 | Step 018/090 | Loss 0.1508 | Time 4.41\n",
            "Epoch 18 | Step 027/090 | Loss 0.1500 | Time 6.62\n",
            "Epoch 18 | Step 036/090 | Loss 0.1417 | Time 8.83\n",
            "Epoch 18 | Step 045/090 | Loss 0.1456 | Time 11.03\n",
            "Epoch 18 | Step 054/090 | Loss 0.1490 | Time 13.24\n",
            "Epoch 18 | Step 063/090 | Loss 0.1552 | Time 15.44\n",
            "Epoch 18 | Step 072/090 | Loss 0.1530 | Time 17.65\n",
            "Epoch 18 | Step 081/090 | Loss 0.1517 | Time 19.86\n",
            "Epoch 18 | Step 090/090 | Loss 0.1512 | Time 22.06\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7606, best pearson is 0.7717\n",
            "Time costed : 23.245s \n",
            "\n",
            "—————————————————————— Epoch 19 ——————————————————————\n",
            "Epoch 19 | Step 009/090 | Loss 0.1628 | Time 2.20\n",
            "Epoch 19 | Step 018/090 | Loss 0.1467 | Time 4.41\n",
            "Epoch 19 | Step 027/090 | Loss 0.1515 | Time 6.62\n",
            "Epoch 19 | Step 036/090 | Loss 0.1527 | Time 8.82\n",
            "Epoch 19 | Step 045/090 | Loss 0.1532 | Time 11.03\n",
            "Epoch 19 | Step 054/090 | Loss 0.1521 | Time 13.24\n",
            "Epoch 19 | Step 063/090 | Loss 0.1526 | Time 15.44\n",
            "Epoch 19 | Step 072/090 | Loss 0.1565 | Time 17.65\n",
            "Epoch 19 | Step 081/090 | Loss 0.1522 | Time 19.86\n",
            "Epoch 19 | Step 090/090 | Loss 0.1517 | Time 22.06\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7553, best pearson is 0.7717\n",
            "Time costed : 23.251s \n",
            "\n",
            "—————————————————————— Epoch 20 ——————————————————————\n",
            "Epoch 20 | Step 009/090 | Loss 0.1684 | Time 2.21\n",
            "Epoch 20 | Step 018/090 | Loss 0.1455 | Time 4.41\n",
            "Epoch 20 | Step 027/090 | Loss 0.1509 | Time 6.62\n",
            "Epoch 20 | Step 036/090 | Loss 0.1461 | Time 8.82\n",
            "Epoch 20 | Step 045/090 | Loss 0.1454 | Time 11.03\n",
            "Epoch 20 | Step 054/090 | Loss 0.1447 | Time 13.24\n",
            "Epoch 20 | Step 063/090 | Loss 0.1454 | Time 15.45\n",
            "Epoch 20 | Step 072/090 | Loss 0.1467 | Time 17.65\n",
            "Epoch 20 | Step 081/090 | Loss 0.1494 | Time 19.86\n",
            "Epoch 20 | Step 090/090 | Loss 0.1503 | Time 22.07\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7622, best pearson is 0.7717\n",
            "Time costed : 23.26s \n",
            "\n",
            "—————————————————————— Epoch 21 ——————————————————————\n",
            "Epoch 21 | Step 009/090 | Loss 0.1787 | Time 2.20\n",
            "Epoch 21 | Step 018/090 | Loss 0.1663 | Time 4.41\n",
            "Epoch 21 | Step 027/090 | Loss 0.1688 | Time 6.62\n",
            "Epoch 21 | Step 036/090 | Loss 0.1583 | Time 8.82\n",
            "Epoch 21 | Step 045/090 | Loss 0.1663 | Time 11.03\n",
            "Epoch 21 | Step 054/090 | Loss 0.1589 | Time 13.23\n",
            "Epoch 21 | Step 063/090 | Loss 0.1557 | Time 15.44\n",
            "Epoch 21 | Step 072/090 | Loss 0.1565 | Time 17.65\n",
            "Epoch 21 | Step 081/090 | Loss 0.1541 | Time 19.85\n",
            "Epoch 21 | Step 090/090 | Loss 0.1522 | Time 22.06\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7706, best pearson is 0.7717\n",
            "Time costed : 23.246s \n",
            "\n",
            "—————————————————————— Epoch 22 ——————————————————————\n",
            "Epoch 22 | Step 009/090 | Loss 0.1569 | Time 2.21\n",
            "Epoch 22 | Step 018/090 | Loss 0.1621 | Time 4.41\n",
            "Epoch 22 | Step 027/090 | Loss 0.1668 | Time 6.62\n",
            "Epoch 22 | Step 036/090 | Loss 0.1574 | Time 8.83\n",
            "Epoch 22 | Step 045/090 | Loss 0.1536 | Time 11.03\n",
            "Epoch 22 | Step 054/090 | Loss 0.1568 | Time 13.24\n",
            "Epoch 22 | Step 063/090 | Loss 0.1534 | Time 15.45\n",
            "Epoch 22 | Step 072/090 | Loss 0.1502 | Time 17.65\n",
            "Epoch 22 | Step 081/090 | Loss 0.1493 | Time 19.86\n",
            "Epoch 22 | Step 090/090 | Loss 0.1530 | Time 22.06\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7730, best pearson is 0.7730\n",
            "Time costed : 25.95s \n",
            "\n",
            "—————————————————————— Epoch 23 ——————————————————————\n",
            "Epoch 23 | Step 009/090 | Loss 0.1429 | Time 2.23\n",
            "Epoch 23 | Step 018/090 | Loss 0.1432 | Time 4.44\n",
            "Epoch 23 | Step 027/090 | Loss 0.1502 | Time 6.64\n",
            "Epoch 23 | Step 036/090 | Loss 0.1400 | Time 8.85\n",
            "Epoch 23 | Step 045/090 | Loss 0.1396 | Time 11.05\n",
            "Epoch 23 | Step 054/090 | Loss 0.1418 | Time 13.26\n",
            "Epoch 23 | Step 063/090 | Loss 0.1418 | Time 15.46\n",
            "Epoch 23 | Step 072/090 | Loss 0.1459 | Time 17.67\n",
            "Epoch 23 | Step 081/090 | Loss 0.1491 | Time 19.87\n",
            "Epoch 23 | Step 090/090 | Loss 0.1508 | Time 22.08\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7651, best pearson is 0.7730\n",
            "Time costed : 23.273s \n",
            "\n",
            "—————————————————————— Epoch 24 ——————————————————————\n",
            "Epoch 24 | Step 009/090 | Loss 0.1459 | Time 2.20\n",
            "Epoch 24 | Step 018/090 | Loss 0.1493 | Time 4.41\n",
            "Epoch 24 | Step 027/090 | Loss 0.1473 | Time 6.62\n",
            "Epoch 24 | Step 036/090 | Loss 0.1543 | Time 8.82\n",
            "Epoch 24 | Step 045/090 | Loss 0.1473 | Time 11.03\n",
            "Epoch 24 | Step 054/090 | Loss 0.1497 | Time 13.23\n",
            "Epoch 24 | Step 063/090 | Loss 0.1488 | Time 15.44\n",
            "Epoch 24 | Step 072/090 | Loss 0.1505 | Time 17.64\n",
            "Epoch 24 | Step 081/090 | Loss 0.1486 | Time 19.85\n",
            "Epoch 24 | Step 090/090 | Loss 0.1487 | Time 22.05\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7758, best pearson is 0.7758\n",
            "Time costed : 26.013s \n",
            "\n",
            "—————————————————————— Epoch 25 ——————————————————————\n",
            "Epoch 25 | Step 009/090 | Loss 0.2016 | Time 2.23\n",
            "Epoch 25 | Step 018/090 | Loss 0.1836 | Time 4.43\n",
            "Epoch 25 | Step 027/090 | Loss 0.1642 | Time 6.64\n",
            "Epoch 25 | Step 036/090 | Loss 0.1601 | Time 8.85\n",
            "Epoch 25 | Step 045/090 | Loss 0.1553 | Time 11.05\n",
            "Epoch 25 | Step 054/090 | Loss 0.1519 | Time 13.26\n",
            "Epoch 25 | Step 063/090 | Loss 0.1523 | Time 15.46\n",
            "Epoch 25 | Step 072/090 | Loss 0.1518 | Time 17.67\n",
            "Epoch 25 | Step 081/090 | Loss 0.1493 | Time 19.87\n",
            "Epoch 25 | Step 090/090 | Loss 0.1492 | Time 22.08\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7734, best pearson is 0.7758\n",
            "Time costed : 23.272s \n",
            "\n",
            "—————————————————————— Epoch 26 ——————————————————————\n",
            "Epoch 26 | Step 009/090 | Loss 0.1449 | Time 2.21\n",
            "Epoch 26 | Step 018/090 | Loss 0.1522 | Time 4.41\n",
            "Epoch 26 | Step 027/090 | Loss 0.1496 | Time 6.62\n",
            "Epoch 26 | Step 036/090 | Loss 0.1462 | Time 8.83\n",
            "Epoch 26 | Step 045/090 | Loss 0.1552 | Time 11.03\n",
            "Epoch 26 | Step 054/090 | Loss 0.1598 | Time 13.24\n",
            "Epoch 26 | Step 063/090 | Loss 0.1574 | Time 15.44\n",
            "Epoch 26 | Step 072/090 | Loss 0.1558 | Time 17.65\n",
            "Epoch 26 | Step 081/090 | Loss 0.1557 | Time 19.85\n",
            "Epoch 26 | Step 090/090 | Loss 0.1572 | Time 22.06\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7980, best pearson is 0.7980\n",
            "Time costed : 26.275s \n",
            "\n",
            "—————————————————————— Epoch 27 ——————————————————————\n",
            "Epoch 27 | Step 009/090 | Loss 0.1511 | Time 2.26\n",
            "Epoch 27 | Step 018/090 | Loss 0.1496 | Time 4.46\n",
            "Epoch 27 | Step 027/090 | Loss 0.1545 | Time 6.67\n",
            "Epoch 27 | Step 036/090 | Loss 0.1505 | Time 8.87\n",
            "Epoch 27 | Step 045/090 | Loss 0.1440 | Time 11.08\n",
            "Epoch 27 | Step 054/090 | Loss 0.1409 | Time 13.29\n",
            "Epoch 27 | Step 063/090 | Loss 0.1450 | Time 15.49\n",
            "Epoch 27 | Step 072/090 | Loss 0.1456 | Time 17.70\n",
            "Epoch 27 | Step 081/090 | Loss 0.1490 | Time 19.90\n",
            "Epoch 27 | Step 090/090 | Loss 0.1521 | Time 22.11\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7833, best pearson is 0.7980\n",
            "Time costed : 23.375s \n",
            "\n",
            "—————————————————————— Epoch 28 ——————————————————————\n",
            "Epoch 28 | Step 009/090 | Loss 0.1399 | Time 2.21\n",
            "Epoch 28 | Step 018/090 | Loss 0.1514 | Time 4.41\n",
            "Epoch 28 | Step 027/090 | Loss 0.1608 | Time 6.62\n",
            "Epoch 28 | Step 036/090 | Loss 0.1601 | Time 8.82\n",
            "Epoch 28 | Step 045/090 | Loss 0.1567 | Time 11.03\n",
            "Epoch 28 | Step 054/090 | Loss 0.1560 | Time 13.23\n",
            "Epoch 28 | Step 063/090 | Loss 0.1493 | Time 15.44\n",
            "Epoch 28 | Step 072/090 | Loss 0.1505 | Time 17.65\n",
            "Epoch 28 | Step 081/090 | Loss 0.1479 | Time 19.85\n",
            "Epoch 28 | Step 090/090 | Loss 0.1484 | Time 22.06\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7866, best pearson is 0.7980\n",
            "Time costed : 23.239s \n",
            "\n",
            "—————————————————————— Epoch 29 ——————————————————————\n",
            "Epoch 29 | Step 009/090 | Loss 0.1153 | Time 2.20\n",
            "Epoch 29 | Step 018/090 | Loss 0.1321 | Time 4.41\n",
            "Epoch 29 | Step 027/090 | Loss 0.1417 | Time 6.62\n",
            "Epoch 29 | Step 036/090 | Loss 0.1544 | Time 8.82\n",
            "Epoch 29 | Step 045/090 | Loss 0.1532 | Time 11.03\n",
            "Epoch 29 | Step 054/090 | Loss 0.1499 | Time 13.24\n",
            "Epoch 29 | Step 063/090 | Loss 0.1463 | Time 15.44\n",
            "Epoch 29 | Step 072/090 | Loss 0.1456 | Time 17.65\n",
            "Epoch 29 | Step 081/090 | Loss 0.1473 | Time 19.85\n",
            "Epoch 29 | Step 090/090 | Loss 0.1474 | Time 22.06\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7894, best pearson is 0.7980\n",
            "Time costed : 23.258s \n",
            "\n",
            "—————————————————————— Epoch 30 ——————————————————————\n",
            "Epoch 30 | Step 009/090 | Loss 0.1386 | Time 2.20\n",
            "Epoch 30 | Step 018/090 | Loss 0.1558 | Time 4.41\n",
            "Epoch 30 | Step 027/090 | Loss 0.1496 | Time 6.62\n",
            "Epoch 30 | Step 036/090 | Loss 0.1458 | Time 8.82\n",
            "Epoch 30 | Step 045/090 | Loss 0.1450 | Time 11.03\n",
            "Epoch 30 | Step 054/090 | Loss 0.1455 | Time 13.23\n",
            "Epoch 30 | Step 063/090 | Loss 0.1438 | Time 15.44\n",
            "Epoch 30 | Step 072/090 | Loss 0.1414 | Time 17.65\n",
            "Epoch 30 | Step 081/090 | Loss 0.1462 | Time 19.85\n",
            "Epoch 30 | Step 090/090 | Loss 0.1466 | Time 22.05\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7751, best pearson is 0.7980\n",
            "Time costed : 23.233s \n",
            "\n",
            "—————————————————————— Epoch 31 ——————————————————————\n",
            "Epoch 31 | Step 009/090 | Loss 0.1241 | Time 2.20\n",
            "Epoch 31 | Step 018/090 | Loss 0.1279 | Time 4.41\n",
            "Epoch 31 | Step 027/090 | Loss 0.1429 | Time 6.61\n",
            "Epoch 31 | Step 036/090 | Loss 0.1451 | Time 8.82\n",
            "Epoch 31 | Step 045/090 | Loss 0.1438 | Time 11.02\n",
            "Epoch 31 | Step 054/090 | Loss 0.1478 | Time 13.23\n",
            "Epoch 31 | Step 063/090 | Loss 0.1466 | Time 15.43\n",
            "Epoch 31 | Step 072/090 | Loss 0.1460 | Time 17.64\n",
            "Epoch 31 | Step 081/090 | Loss 0.1431 | Time 19.84\n",
            "Epoch 31 | Step 090/090 | Loss 0.1430 | Time 22.04\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7768, best pearson is 0.7980\n",
            "Time costed : 23.247s \n",
            "\n",
            "—————————————————————— Epoch 32 ——————————————————————\n",
            "Epoch 32 | Step 009/090 | Loss 0.1396 | Time 2.21\n",
            "Epoch 32 | Step 018/090 | Loss 0.1331 | Time 4.41\n",
            "Epoch 32 | Step 027/090 | Loss 0.1309 | Time 6.62\n",
            "Epoch 32 | Step 036/090 | Loss 0.1247 | Time 8.82\n",
            "Epoch 32 | Step 045/090 | Loss 0.1295 | Time 11.02\n",
            "Epoch 32 | Step 054/090 | Loss 0.1317 | Time 13.23\n",
            "Epoch 32 | Step 063/090 | Loss 0.1316 | Time 15.44\n",
            "Epoch 32 | Step 072/090 | Loss 0.1360 | Time 17.64\n",
            "Epoch 32 | Step 081/090 | Loss 0.1355 | Time 19.85\n",
            "Epoch 32 | Step 090/090 | Loss 0.1412 | Time 22.05\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7751, best pearson is 0.7980\n",
            "Time costed : 23.238s \n",
            "\n",
            "—————————————————————— Epoch 33 ——————————————————————\n",
            "Epoch 33 | Step 009/090 | Loss 0.1704 | Time 2.21\n",
            "Epoch 33 | Step 018/090 | Loss 0.1581 | Time 4.41\n",
            "Epoch 33 | Step 027/090 | Loss 0.1586 | Time 6.62\n",
            "Epoch 33 | Step 036/090 | Loss 0.1470 | Time 8.82\n",
            "Epoch 33 | Step 045/090 | Loss 0.1457 | Time 11.02\n",
            "Epoch 33 | Step 054/090 | Loss 0.1466 | Time 13.23\n",
            "Epoch 33 | Step 063/090 | Loss 0.1414 | Time 15.44\n",
            "Epoch 33 | Step 072/090 | Loss 0.1394 | Time 17.64\n",
            "Epoch 33 | Step 081/090 | Loss 0.1393 | Time 19.85\n",
            "Epoch 33 | Step 090/090 | Loss 0.1404 | Time 22.05\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7745, best pearson is 0.7980\n",
            "Time costed : 23.237s \n",
            "\n",
            "—————————————————————— Epoch 34 ——————————————————————\n",
            "Epoch 34 | Step 009/090 | Loss 0.1344 | Time 2.21\n",
            "Epoch 34 | Step 018/090 | Loss 0.1542 | Time 4.41\n",
            "Epoch 34 | Step 027/090 | Loss 0.1553 | Time 6.62\n",
            "Epoch 34 | Step 036/090 | Loss 0.1481 | Time 8.82\n",
            "Epoch 34 | Step 045/090 | Loss 0.1453 | Time 11.03\n",
            "Epoch 34 | Step 054/090 | Loss 0.1449 | Time 13.23\n",
            "Epoch 34 | Step 063/090 | Loss 0.1405 | Time 15.44\n",
            "Epoch 34 | Step 072/090 | Loss 0.1425 | Time 17.64\n",
            "Epoch 34 | Step 081/090 | Loss 0.1414 | Time 19.85\n",
            "Epoch 34 | Step 090/090 | Loss 0.1406 | Time 22.05\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7752, best pearson is 0.7980\n",
            "Time costed : 23.269s \n",
            "\n",
            "—————————————————————— Epoch 35 ——————————————————————\n",
            "Epoch 35 | Step 009/090 | Loss 0.1681 | Time 2.21\n",
            "Epoch 35 | Step 018/090 | Loss 0.1655 | Time 4.41\n",
            "Epoch 35 | Step 027/090 | Loss 0.1548 | Time 6.61\n",
            "Epoch 35 | Step 036/090 | Loss 0.1527 | Time 8.82\n",
            "Epoch 35 | Step 045/090 | Loss 0.1458 | Time 11.02\n",
            "Epoch 35 | Step 054/090 | Loss 0.1460 | Time 13.23\n",
            "Epoch 35 | Step 063/090 | Loss 0.1449 | Time 15.44\n",
            "Epoch 35 | Step 072/090 | Loss 0.1429 | Time 17.64\n",
            "Epoch 35 | Step 081/090 | Loss 0.1448 | Time 19.85\n",
            "Epoch 35 | Step 090/090 | Loss 0.1403 | Time 22.05\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7817, best pearson is 0.7980\n",
            "Time costed : 23.226s \n",
            "\n",
            "—————————————————————— Epoch 36 ——————————————————————\n",
            "Epoch 36 | Step 009/090 | Loss 0.1758 | Time 2.21\n",
            "Epoch 36 | Step 018/090 | Loss 0.1467 | Time 4.41\n",
            "Epoch 36 | Step 027/090 | Loss 0.1467 | Time 6.62\n",
            "Epoch 36 | Step 036/090 | Loss 0.1447 | Time 8.82\n",
            "Epoch 36 | Step 045/090 | Loss 0.1452 | Time 11.03\n",
            "Epoch 36 | Step 054/090 | Loss 0.1411 | Time 13.23\n",
            "Epoch 36 | Step 063/090 | Loss 0.1399 | Time 15.44\n",
            "Epoch 36 | Step 072/090 | Loss 0.1425 | Time 17.64\n",
            "Epoch 36 | Step 081/090 | Loss 0.1419 | Time 19.85\n",
            "Epoch 36 | Step 090/090 | Loss 0.1423 | Time 22.05\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7692, best pearson is 0.7980\n",
            "Time costed : 23.257s \n",
            "\n",
            "—————————————————————— Epoch 37 ——————————————————————\n",
            "Epoch 37 | Step 009/090 | Loss 0.1127 | Time 2.20\n",
            "Epoch 37 | Step 018/090 | Loss 0.1464 | Time 4.41\n",
            "Epoch 37 | Step 027/090 | Loss 0.1416 | Time 6.62\n",
            "Epoch 37 | Step 036/090 | Loss 0.1487 | Time 8.82\n",
            "Epoch 37 | Step 045/090 | Loss 0.1407 | Time 11.03\n",
            "Epoch 37 | Step 054/090 | Loss 0.1426 | Time 13.23\n",
            "Epoch 37 | Step 063/090 | Loss 0.1433 | Time 15.44\n",
            "Epoch 37 | Step 072/090 | Loss 0.1423 | Time 17.64\n",
            "Epoch 37 | Step 081/090 | Loss 0.1427 | Time 19.85\n",
            "Epoch 37 | Step 090/090 | Loss 0.1412 | Time 22.06\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7710, best pearson is 0.7980\n",
            "Time costed : 23.235s \n",
            "\n",
            "—————————————————————— Epoch 38 ——————————————————————\n",
            "Epoch 38 | Step 009/090 | Loss 0.1369 | Time 2.20\n",
            "Epoch 38 | Step 018/090 | Loss 0.1359 | Time 4.41\n",
            "Epoch 38 | Step 027/090 | Loss 0.1435 | Time 6.61\n",
            "Epoch 38 | Step 036/090 | Loss 0.1391 | Time 8.82\n",
            "Epoch 38 | Step 045/090 | Loss 0.1362 | Time 11.02\n",
            "Epoch 38 | Step 054/090 | Loss 0.1377 | Time 13.23\n",
            "Epoch 38 | Step 063/090 | Loss 0.1377 | Time 15.44\n",
            "Epoch 38 | Step 072/090 | Loss 0.1400 | Time 17.64\n",
            "Epoch 38 | Step 081/090 | Loss 0.1393 | Time 19.84\n",
            "Epoch 38 | Step 090/090 | Loss 0.1394 | Time 22.05\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7772, best pearson is 0.7980\n",
            "Time costed : 23.258s \n",
            "\n",
            "—————————————————————— Epoch 39 ——————————————————————\n",
            "Epoch 39 | Step 009/090 | Loss 0.1173 | Time 2.21\n",
            "Epoch 39 | Step 018/090 | Loss 0.1342 | Time 4.41\n",
            "Epoch 39 | Step 027/090 | Loss 0.1339 | Time 6.62\n",
            "Epoch 39 | Step 036/090 | Loss 0.1404 | Time 8.83\n",
            "Epoch 39 | Step 045/090 | Loss 0.1338 | Time 11.03\n",
            "Epoch 39 | Step 054/090 | Loss 0.1281 | Time 13.24\n",
            "Epoch 39 | Step 063/090 | Loss 0.1304 | Time 15.44\n",
            "Epoch 39 | Step 072/090 | Loss 0.1304 | Time 17.65\n",
            "Epoch 39 | Step 081/090 | Loss 0.1346 | Time 19.85\n",
            "Epoch 39 | Step 090/090 | Loss 0.1384 | Time 22.06\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7803, best pearson is 0.7980\n",
            "Time costed : 23.231s \n",
            "\n",
            "—————————————————————— Epoch 40 ——————————————————————\n",
            "Epoch 40 | Step 009/090 | Loss 0.1362 | Time 2.21\n",
            "Epoch 40 | Step 018/090 | Loss 0.1251 | Time 4.41\n",
            "Epoch 40 | Step 027/090 | Loss 0.1244 | Time 6.61\n",
            "Epoch 40 | Step 036/090 | Loss 0.1303 | Time 8.82\n",
            "Epoch 40 | Step 045/090 | Loss 0.1312 | Time 11.02\n",
            "Epoch 40 | Step 054/090 | Loss 0.1350 | Time 13.23\n",
            "Epoch 40 | Step 063/090 | Loss 0.1373 | Time 15.43\n",
            "Epoch 40 | Step 072/090 | Loss 0.1356 | Time 17.64\n",
            "Epoch 40 | Step 081/090 | Loss 0.1390 | Time 19.85\n",
            "Epoch 40 | Step 090/090 | Loss 0.1385 | Time 22.05\n",
            "Start evaluating!\n",
            "Current dev pearson is 0.7337, best pearson is 0.7980\n",
            "Time costed : 23.239s \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run model finetuning and save fine-tuned model.\n",
        "# pre_trained_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-large\")\n",
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "pre_trained_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
        "config = XLMRobertaConfig.from_pretrained(\"xlm-roberta-base\")\n",
        "hidden_size = config.hidden_size\n",
        "# hidden_size = 768\n",
        "\n",
        "loss_weights = [0.5 if i == 4 else (1-0.5)/6 for i in range(7)]\n",
        "\n",
        "model_name = f'XLM_Roberta_base.pth'\n",
        "model_path = f\"/content/drive/MyDrive/CS263/Final Project/Models/{model_name}\"\n",
        "\n",
        "model = MMRegressor(pre_trained_model, hidden_size)\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model name for this run: {model_name}\")\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
        "train(model, model_path, train_data_loader, eval_data_loader, optimizer, num_epochs, loss_weights)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOQ4RzesGSSP",
        "outputId": "cc24d7ae-defe-403a-db6f-a4a7b968eeef"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_data.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "l_LJq1mNgInH",
        "outputId": "1a07e206-193d-4413-ef52-36131e2e7d5a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  url1_lang url2_lang                pair_id  \\\n",
              "0        en        en  1484189203_1484121193   \n",
              "1        en        en  1484011097_1484011106   \n",
              "\n",
              "                                               link1  \\\n",
              "0  https://wsvn.com/news/local/broward/police-2-m...   \n",
              "1  https://www.zdnet.com/article/autoclerk-databa...   \n",
              "\n",
              "                                               link2  \\\n",
              "0  https://wsvn.com/news/local/no-swim-advisory-l...   \n",
              "1  https://securityboulevard.com/2019/10/best-wes...   \n",
              "\n",
              "                                            ia_link1  \\\n",
              "0  https://web.archive.org/web/https://wsvn.com/n...   \n",
              "1  https://web.archive.org/web/https://www.zdnet....   \n",
              "\n",
              "                                            ia_link2  GEO  ENT  TIME  NAR  \\\n",
              "0  https://web.archive.org/web/https://wsvn.com/n...  1.5  4.0   2.0  4.0   \n",
              "1  https://web.archive.org/web/https://securitybo...  1.0  2.0   1.0  1.0   \n",
              "\n",
              "   Overall  STYLE  TONE                                              text1  \\\n",
              "0      3.5    1.0   1.5  DAVIE, FLA. (WSVN) - Police need help catching...   \n",
              "1      1.0    3.5   2.5  Most Popular An open database exposing records...   \n",
              "\n",
              "                                               text2  \n",
              "0  DEERFIELD BEACH, FLA. (WSVN) - A no-swim advis...  \n",
              "1  The Home of the Security Bloggers Network Home...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-02ee2d60-a00a-4984-9c05-8b8e99f0b269\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url1_lang</th>\n",
              "      <th>url2_lang</th>\n",
              "      <th>pair_id</th>\n",
              "      <th>link1</th>\n",
              "      <th>link2</th>\n",
              "      <th>ia_link1</th>\n",
              "      <th>ia_link2</th>\n",
              "      <th>GEO</th>\n",
              "      <th>ENT</th>\n",
              "      <th>TIME</th>\n",
              "      <th>NAR</th>\n",
              "      <th>Overall</th>\n",
              "      <th>STYLE</th>\n",
              "      <th>TONE</th>\n",
              "      <th>text1</th>\n",
              "      <th>text2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>1484189203_1484121193</td>\n",
              "      <td>https://wsvn.com/news/local/broward/police-2-m...</td>\n",
              "      <td>https://wsvn.com/news/local/no-swim-advisory-l...</td>\n",
              "      <td>https://web.archive.org/web/https://wsvn.com/n...</td>\n",
              "      <td>https://web.archive.org/web/https://wsvn.com/n...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>DAVIE, FLA. (WSVN) - Police need help catching...</td>\n",
              "      <td>DEERFIELD BEACH, FLA. (WSVN) - A no-swim advis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>1484011097_1484011106</td>\n",
              "      <td>https://www.zdnet.com/article/autoclerk-databa...</td>\n",
              "      <td>https://securityboulevard.com/2019/10/best-wes...</td>\n",
              "      <td>https://web.archive.org/web/https://www.zdnet....</td>\n",
              "      <td>https://web.archive.org/web/https://securitybo...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>Most Popular An open database exposing records...</td>\n",
              "      <td>The Home of the Security Bloggers Network Home...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02ee2d60-a00a-4984-9c05-8b8e99f0b269')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-02ee2d60-a00a-4984-9c05-8b8e99f0b269 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-02ee2d60-a00a-4984-9c05-8b8e99f0b269');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete columns that contain NaN sentences.\n",
        "processed_test_data = dev_data[dev_data['text1'].notna()]\n",
        "processed_test_data = processed_test_data[dev_data['text2'].notna()]\n",
        "\n",
        "print(\"After removing NA text columns, we lose {0} rows.\".format(dev_data.shape[0] - processed_test_data.shape[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IGCGIBcinkE",
        "outputId": "58b02e9f-34c4-4c54-f1a3-fd25a41f0f29"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After removing NA text columns, we lose 0 rows.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_test_data['Geography'] = processed_test_data['GEO']\n",
        "del processed_test_data['GEO']\n",
        "processed_test_data['Entities'] = processed_test_data.pop('ENT')\n",
        "processed_test_data['Time'] = processed_test_data.pop('TIME')\n",
        "processed_test_data['Narrative'] = processed_test_data.pop('NAR')\n",
        "processed_test_data['Overall'] = processed_test_data.pop('Overall')\n",
        "processed_test_data['Style'] = processed_test_data.pop('STYLE')\n",
        "processed_test_data['Tone'] = processed_test_data.pop('TONE')\n",
        "processed_test_data['text1'] = processed_test_data.pop('text1')\n",
        "processed_test_data['text2'] = processed_test_data.pop('text2')"
      ],
      "metadata": {
        "id": "LC5lYjOei7yr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_loader = get_data_loader(processed_test_data, False)"
      ],
      "metadata": {
        "id": "zEIFyrQ6i_GT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(processed_test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwaeoKdnXF0d",
        "outputId": "2d7733b3-e366-41dc-98f9-baf6e2c8ed88"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4902"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = XLMRobertaConfig.from_pretrained(\"xlm-roberta-base\")\n",
        "hidden_size = config.hidden_size\n",
        "\n",
        "pre_trained_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
        "model = MMRegressor(pre_trained_model, hidden_size)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/CS263/Final Project/Models/XLM_Roberta_base.pth\"), strict=False)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WsmhAEHjHfj",
        "outputId": "c9d367de-3ba5-4faa-f9fd-5d52d1a6627b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MMRegressor(\n",
              "  (reg_model): XLMRobertaModel(\n",
              "    (embeddings): XLMRobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): XLMRobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): XLMRobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=7, bias=True)\n",
              "  (activation): GELU(approximate='none')\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tBqehpqGVic",
        "outputId": "ba5ba5cc-7c3d-4569-fa7d-52c0d92d3833"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "fI7kgjz6wTMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_overall, test_true_overall = predict(model, test_data_loader)\n",
        "test_pearson_score = np.corrcoef(test_pred_overall, test_true_overall)[0][1]\n",
        "\n",
        "print(\"Pearson score on test dataset is {:.3f}\".format(test_pearson_score))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmqwwzuujNck",
        "outputId": "7b788f63-8b16-4a2b-da0e-826fe5f3514a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson score on test dataset is 0.384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_all = get_data_loader(processed_data, False)\n",
        "train_pred_overall, train_true_overall = predict(model, train_all)\n",
        "train_pearson_score = np.corrcoef(train_pred_overall, train_true_overall)[0][1]\n",
        "print(\"Pearson score on entire train dataset is {:.3f}\".format(train_pearson_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urvLbgkVrU9D",
        "outputId": "3509b241-34c6-4b94-8ede-f7bb62bf7b02"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson score on entire train dataset is 0.967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TonyX"
      ],
      "metadata": {
        "id": "KZgjhEv59Asa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "w8nitLh-C-GA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import XLMRobertaConfig, XLMRobertaModel, XLMRobertaTokenizer, AdamW, get_cosine_schedule_with_warmup\n",
        "\n",
        "def set_seed(seed=56):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED']=str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "class MMRegressor(nn.Module):\n",
        "\n",
        "    def __init__(self, model_path):\n",
        "        \n",
        "        super(MMRegressor, self).__init__()\n",
        "        self.config = XLMRobertaConfig.from_pretrained(model_path)\n",
        "        self.reg_model = XLMRobertaModel.from_pretrained(model_path)\n",
        "\n",
        "        self.fc1 = nn.Linear(self.config.hidden_size, 512)\n",
        "        self.fc2 = nn.Linear(512, 7)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "\n",
        "        output1 = self.reg_model(input_ids, attention_mask)[1]\n",
        "        logits1 = self.fc2(self.activation(self.fc1(output1)))\n",
        "\n",
        "        output2 = self.reg_model(input_ids, attention_mask)[1]\n",
        "        logits2 = self.fc2(self.activation(self.fc1(output2)))\n",
        "        \n",
        "        return logits1, logits2\n",
        "\n",
        "class Reg_FT_Configer():\n",
        "\n",
        "    def __init__(self, params_dict: dict):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.learning_rate = params_dict['learning_rate']\n",
        "        self.epoch =params_dict['epoch']\n",
        "        self.gradient_acc = params_dict['gradient_acc']\n",
        "        self.batch_size = params_dict['batch_size']\n",
        "        self.max_len = params_dict['max_len']\n",
        "        self.model_save_path = params_dict['model_save_path']\n",
        "        self.warmup_rate = params_dict['warmup_rate']\n",
        "        self.weight_decay = params_dict['weight_decay']\n",
        "        self.model_pretrain_dir = params_dict['model_pretrain_dir']\n",
        "        self.training_set_path = params_dict['training_set_path']\n",
        "        self.testing_set_path = params_dict['testing_set_path']\n",
        "        self.seed = params_dict['seed']\n",
        "\n",
        "        # weights for the 7 sub-dimensions\n",
        "        self.dims_weights = [params_dict['overall_weight'] if i == 4 else (1-params_dict['overall_weight'])/6 for i in range(7)]\n",
        "        \n",
        "        # weights for forward loss and adapted R-Drop loss\n",
        "        self.losses_weights = {\n",
        "            'forward_weight': (1-params_dict['rdrop_weight'])/2,\n",
        "            'rdrop_weight': params_dict['rdrop_weight']\n",
        "        }\n",
        "        \n",
        "        \n",
        "class Reg_Trainer():\n",
        "\n",
        "    def __init__(self, config: Reg_FT_Configer):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\")\n",
        "        self.tokenizer = XLMRobertaTokenizer.from_pretrained(self.config.model_pretrain_dir)\n",
        "\n",
        "        set_seed(self.config.seed)\n",
        "\n",
        "\n",
        "    def dataset(self, data_path):\n",
        "\n",
        "        input_ids, attention_masks, labels = [], [], []\n",
        "\n",
        "        for idx, row in pd.read_csv(data_path).iterrows():\n",
        "            text1, text2 = row['text1'], row['text2']\n",
        "            encode_dict = self.tokenizer.__call__(text1,text2,\n",
        "                                                    max_length=self.config.max_len,\n",
        "                                                    padding='max_length',\n",
        "                                                    truncation=True,\n",
        "                                                    add_special_tokens=True\n",
        "                                                    )\n",
        "            input_ids.append(encode_dict['input_ids'])\n",
        "            attention_masks.append(encode_dict['attention_mask'])\n",
        "            labels.append([float(x) for x in [row['Geography'],row['Entities'],row['Time'],row['Narrative'],row['Overall'],row['Style'],row['Tone']]])\n",
        "        \n",
        "        return torch.tensor(input_ids), torch.tensor(attention_masks), torch.tensor(labels)\n",
        "    \n",
        "    \n",
        "    def data_loader(self, input_ids, attention_masks, labels):\n",
        "\n",
        "        data = TensorDataset(input_ids, attention_masks, labels)\n",
        "        loader = DataLoader(data, batch_size=self.config.batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "        return loader\n",
        "\n",
        "    def predict(self, model, data_loader):\n",
        "\n",
        "        model.eval()\n",
        "        test_pred, test_true = [], []\n",
        "        with torch.no_grad():\n",
        "            for idx, (ids, att, y) in enumerate(data_loader):\n",
        "                y_pred = model(ids.to(self.device), att.to(self.device))\n",
        "                y_pred = torch.squeeze(torch.add(torch.mul(y_pred[0], 0.5), torch.mul(y_pred[1], 0.5))).detach().cpu().numpy().tolist()\n",
        "                y = y.squeeze().cpu().numpy().tolist()\n",
        "\n",
        "                test_true.extend([x[4] for x in y])\n",
        "                test_pred.extend([x[4] for x in y_pred])\n",
        "\n",
        "            return test_true, test_pred\n",
        "\n",
        "\n",
        "    def calculate_weighted_loss(self, y_pred, y, criterion):\n",
        "\n",
        "        loss = 0.0\n",
        "        for i in range(7):\n",
        "            y_pred_i, y_i = y_pred[:, i], y[:, i]\n",
        "            loss += criterion(y_pred_i, y_i) * self.config.dims_weights[i]\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def train(self, model, train_loader, valid_loader, optimizer, schedule):\n",
        "\n",
        "        best_pearson = 0.0\n",
        "        criterion = nn.MSELoss()\n",
        "        model.train()\n",
        "\n",
        "        for i in range(self.config.epoch):\n",
        "            start_time = time.time()\n",
        "            train_loss_sum = 0.0\n",
        "            \n",
        "            logging.info(f\"—————————————————————— Epoch {i+1} ——————————————————————\")\n",
        "            \n",
        "            for idx, (ids, att, y) in enumerate(train_loader):\n",
        "\n",
        "                ids, att, y = ids.to(self.device), att.to(self.device), y.to(self.device)\n",
        "                y_pred1, y_pred2 = model(ids, att)\n",
        "                y_pred1, y_pred2, y = torch.squeeze(y_pred1), torch.squeeze(y_pred2), torch.squeeze(y)\n",
        "\n",
        "                loss1 = self.calculate_weighted_loss(y_pred1, y, criterion) * self.config.losses_weights['forward_weight']\n",
        "                loss2 = self.calculate_weighted_loss(y_pred2, y, criterion) * self.config.losses_weights['forward_weight']\n",
        "                loss_r = self.calculate_weighted_loss(y_pred1, y_pred2, criterion) * self.config.losses_weights['rdrop_weight']     \n",
        "                loss = (loss1 + loss2 + loss_r) / self.config.gradient_acc\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                schedule.step()\n",
        "                train_loss_sum += loss.item()\n",
        "\n",
        "                if (idx+1) % (len(train_loader) // 10) == 0:\n",
        "                    logging.info(\"Epoch {:02d} | Step {:03d}/{:03d} | Loss {:.4f} | Time {:.2f}\".format(i+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time()-start_time))\n",
        "            \n",
        "\n",
        "            logging.info(\"Start evaluating!\")\n",
        "            dev_true, dev_pred = self.predict(model, valid_loader)\n",
        "            cur_pearson = np.corrcoef(dev_true, dev_pred)[0][1]\n",
        "            \n",
        "            if cur_pearson > best_pearson:\n",
        "                best_pearson = cur_pearson\n",
        "                torch.save(model.state_dict(), self.config.model_save_path)\n",
        "            \n",
        "            logging.info(\"Current dev pearson is {:.4f}, best pearson is {:.4f}\".format(cur_pearson, best_pearson))\n",
        "            logging.info(\"Time costed : {}s \\n\".format(round(time.time() - start_time, 3)))\n",
        "    \n",
        "\n",
        "    def run_finetune(self):\n",
        "\n",
        "        train_loader = self.data_loader(*self.dataset(self.config.training_set_path))\n",
        "        dev_loader = self.data_loader(*self.dataset(self.config.testing_set_path))\n",
        "\n",
        "        model = MMRegressor(self.config.model_pretrain_dir).to(self.device)\n",
        "        \n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        total_steps = len(train_loader) * self.config.epoch\n",
        "\n",
        "        optimizer = AdamW(params=model.parameters(), \n",
        "                        lr=self.config.learning_rate, \n",
        "                        weight_decay=self.config.weight_decay)      \n",
        "        schedule = get_cosine_schedule_with_warmup(optimizer=optimizer,\n",
        "                                                    num_warmup_steps=self.config.warmup_rate*total_steps,\n",
        "                                                    num_training_steps=total_steps)\n",
        "        \n",
        "        self.train(model, train_loader, dev_loader, optimizer, schedule)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}